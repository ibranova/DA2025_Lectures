{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mod8L5 PySpark vs. pandas (Local Parquet) — Do in Google Colab\n",
        "**Goal:** See the syntax and workflow differences between **pandas** and **PySpark** using a **local Parquet** file (no cloud).  \n",
        "**Format:** INSTRUCTOR → YOU DO → REFLECTION"
      ],
      "metadata": {
        "id": "6XmQIRgBnwzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INSTRUCTOR — Follow Along Together (20 mins.)\n",
        "\n",
        "### Why Parquet (local-first)\n",
        "- **Columnar** → fast column scans & smaller I/O.\n",
        "- **Compressed & splittable** → efficient storage & parallel reads.\n",
        "- **Self-describing schema** → types are embedded in the file.\n",
        "- **Predicate pushdown** (Spark) → skip non-matching row groups.\n",
        "\n",
        "**Docs:**  \n",
        "- Spark SQL/DataFrames: https://spark.apache.org/docs/latest/sql-programming-guide.html  \n",
        "- Spark Reader/Writer Parquet: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html  \n",
        "- pandas `read_parquet`/`to_parquet`: https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html"
      ],
      "metadata": {
        "id": "x66en8Npn1Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0 — (Optional) Install libraries (only if needed)\n"
      ],
      "metadata": {
        "id": "mPmlXJpZn5_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S51nZas6nqeK"
      },
      "outputs": [],
      "source": [
        "# Do a Pip Install in the Terminal for these pacakages\n",
        "\n",
        "#pip install pyspark==3.5.1\n",
        "#pip install pyarrow==15.0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 — Create a tiny local Parquet (once) and keep it on disk\n",
        "We’ll generate a small sample dataset, save it to **`./customers.parquet/`** (a folder of Parquet part files)."
      ],
      "metadata": {
        "id": "1va0bpijn-IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL WITHOUT CHANGES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "n = 20\n",
        "pdf = pd.DataFrame({\n",
        "    \"customer_id\": np.arange(1, n+1),\n",
        "    \"age\": rng.integers(18, 70, size=n),\n",
        "    \"country\": rng.choice([\"US\", \"UK\", \"FR\", \"DE\"], size=n),\n",
        "    \"spend_usd\": rng.normal(100, 30, size=n).round(2)\n",
        "})\n",
        "\n",
        "# Save locally as Parquet (folder)\n",
        "None\n",
        "print(\"Wrote local parquet folder: ./customers.parquet/\")\n"
      ],
      "metadata": {
        "id": "NFFFvOVzn-3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf2 = pd.read_parquet(\"customers.parquet\")\n",
        "display(pdf2.head())\n",
        "display(pdf2.dtypes)\n",
        "\n",
        "pdf_country = (\n",
        "    pdf2.groupby(\"country\", as_index=False)\n",
        "        .agg(n_rows=(\"customer_id\",\"count\"),\n",
        "             avg_spend=(\"spend_usd\",\"mean\"))\n",
        "        .sort_values(\"avg_spend\", ascending=False)\n",
        ")\n",
        "display(pdf_country)\n"
      ],
      "metadata": {
        "id": "vAZhDaKIoKAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 — Start SparkSession (local) and read the same local Parquet\n",
        "\n",
        "**Instructor:** You can hover over methods in Google Colab to see the documentation.  Note to students what is similar between this syntax and pandas syntax\n"
      ],
      "metadata": {
        "id": "3LPVr06FoM9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL WITHOUT CHANGES\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = None\n",
        "df = None\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_60PBkT-oNis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 — Spark: do the same (select, filter, groupBy)\n"
      ],
      "metadata": {
        "id": "6v5vNsV8phSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a few columns\n",
        "df_sel = None\n",
        "\n",
        "# Filter\n",
        "df_us = None\n",
        "print(\"US rows:\", df_us.count())\n",
        "df_us.show(truncate=False)\n",
        "\n",
        "# Group & aggregate (Spark)\n",
        "df_country = None\n",
        "df_country.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "fHVRVtMApinx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 — Write a filtered subset to Parquet & round-trip\n"
      ],
      "metadata": {
        "id": "AHCNvizgpr6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = \"customers_high_spend.parquet\"\n",
        "(\n",
        "    df.filter(F.col(\"spend_usd\") > 100)\n",
        "      .write.mode(\"overwrite\")\n",
        "      .parquet(out_path)\n",
        ")\n",
        "rt = spark.read.parquet(out_path)\n",
        "rt.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "cjcer1GIpscX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU DO (30 mins)\n",
        "\n",
        "Use the docs to complete the tasks in Spark **and** (where it helps you think) pandas.\n",
        "\n",
        "**Helpful Docs:**  \n",
        "- Spark `functions`: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html  \n",
        "- Spark `DataFrame` API: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html  \n",
        "- pandas groupby: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
        "\n",
        "### Task A — Schema, sample, and simple transforms (Spark)\n",
        "1. Print the schema and count rows.  \n",
        "2. Add two columns:\n",
        "   - `spend_eur = spend_usd * 0.92`\n",
        "   - `is_senior = age >= 40` (boolean)\n",
        "3. Show the top 8 rows sorted by `spend_usd` descending.\n",
        "\n",
        "### Task B — Aggregations (Spark)\n",
        "4. By `country`, compute:\n",
        "   - `n_rows = count(*)`\n",
        "   - `avg_age = avg(age)`\n",
        "   - `p90_spend = percentile_approx(spend_usd, 0.90)` (use `F.expr(...)` or SQL expr)\n",
        "   Sort by `p90_spend` descending, show all rows.\n",
        "\n",
        "### Task C — (Compare) Do the same aggregation in pandas\n",
        "5. Load the same `customers.parquet` into pandas and compute:\n",
        "   - `n_rows`, `avg_age`, and the 90th percentile of `spend_usd` (use `quantile(0.90)`).\n",
        "   - Sort to match Spark output.\n",
        "\n",
        "> Tip: Notice Spark’s **lazy** execution vs pandas’ **eager** execution.\n"
      ],
      "metadata": {
        "id": "84TQKyytp_yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answers (Instructor Only)"
      ],
      "metadata": {
        "id": "4oxivQ3zqUCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer\n",
        "\n",
        "# YOU DO — write your Spark code below\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# A1) schema & count\n",
        "# df.printSchema()\n",
        "# print(\"Rows:\", df.count())\n",
        "\n",
        "# A2) new columns\n",
        "# df2 = (df\n",
        "#        .withColumn(\"spend_eur\", F.col(\"spend_usd\") * F.lit(0.92))\n",
        "#        .withColumn(\"is_senior\", F.col(\"age\") >= F.lit(40)))\n",
        "# df2.show(5, truncate=False)\n",
        "\n",
        "# A3) sort\n",
        "# df2.orderBy(F.col(\"spend_usd\").desc()).show(8, truncate=False)\n",
        "\n",
        "# B4) aggregations with percentile_approx\n",
        "# country_stats = (df\n",
        "#     .groupBy(\"country\")\n",
        "#     .agg(F.count(\"*\").alias(\"n_rows\"),\n",
        "#          F.avg(\"age\").alias(\"avg_age\"),\n",
        "#          F.expr(\"percentile_approx(spend_usd, 0.90)\").alias(\"p90_spend\"))\n",
        "#     .orderBy(F.col(\"p90_spend\").desc())\n",
        "# )\n",
        "# country_stats.show(truncate=False)\n",
        "# YOU DO — pandas comparison for Task C\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the same local parquet into pandas\n",
        "# pdf3 = pd.read_parquet(\"customers.parquet\")\n",
        "\n",
        "# agg with pandas\n",
        "# pdf_stats = (pdf3\n",
        "#              .groupby(\"country\", as_index=False)\n",
        "#              .agg(n_rows=(\"customer_id\",\"count\"),\n",
        "#                   avg_age=(\"age\",\"mean\"),\n",
        "#                   p90_spend=(\"spend_usd\", lambda s: s.quantile(0.90)))\n",
        "#              .sort_values(\"p90_spend\", ascending=False)\n",
        "#             )\n",
        "# display(pdf_stats)\n",
        "\n"
      ],
      "metadata": {
        "id": "NdWqmYXnp13y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL WITHOUT CHANGES -- IMPORTANT\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ],
      "metadata": {
        "id": "JHnG3EesqhGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REFLECTION — Short Answers\n",
        "\n",
        "\n",
        "1) **Why Parquet locally?**\n",
        "- If you used CSV yesterday and Parquet today for the same data, what changes in performance and why?\n",
        "\n",
        "2) **Scaling Up**\n",
        "- If this dataset were 100 GB on your laptop, which tool would likely still run and why?  \n",
        "- What would you change first to handle that scale?\n",
        "\n"
      ],
      "metadata": {
        "id": "S-sLGothqb0Z"
      }
    }
  ]
}